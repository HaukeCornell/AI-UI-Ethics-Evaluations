{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human vs Model UX Assessment Comparison\n",
    "\n",
    "This notebook compares human evaluations with model evaluations for dark patterns, focusing on UX KPI metrics and pattern assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# Set up visual style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"temp\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First, let's load both human evaluations and model evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human data shape: (348, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata_participant_id</th>\n",
       "      <th>metadata_timestamp</th>\n",
       "      <th>metadata_pattern_type</th>\n",
       "      <th>metadata_interface_id</th>\n",
       "      <th>score_inefficient_efficient</th>\n",
       "      <th>score_interesting_not_interesting</th>\n",
       "      <th>score_clear_confusing</th>\n",
       "      <th>score_enjoyable_annoying</th>\n",
       "      <th>score_organized_cluttered</th>\n",
       "      <th>score_addictive_non_addictive</th>\n",
       "      <th>score_supportive_obstructive</th>\n",
       "      <th>score_pressuring_suggesting</th>\n",
       "      <th>score_boring_exciting</th>\n",
       "      <th>score_revealed_covert</th>\n",
       "      <th>score_complicated_easy</th>\n",
       "      <th>score_unpredictable_predictable</th>\n",
       "      <th>score_friendly_unfriendly</th>\n",
       "      <th>score_deceptive_benevolent</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P002</td>\n",
       "      <td>2025-04-04T10:06:23</td>\n",
       "      <td>Overcomplicated Process</td>\n",
       "      <td>interface_002</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002</td>\n",
       "      <td>2025-04-04T10:06:25</td>\n",
       "      <td>Sneaking Bad Default</td>\n",
       "      <td>interface_004</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P002</td>\n",
       "      <td>2025-04-04T10:06:29</td>\n",
       "      <td>Toying With Emotion</td>\n",
       "      <td>interface_008</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P002</td>\n",
       "      <td>2025-04-04T10:06:35</td>\n",
       "      <td>Endlessness</td>\n",
       "      <td>interface_014</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P003</td>\n",
       "      <td>2025-04-04T10:06:26</td>\n",
       "      <td>Expectation Result Mismatch</td>\n",
       "      <td>interface_005</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  metadata_participant_id   metadata_timestamp        metadata_pattern_type  \\\n",
       "0                    P002  2025-04-04T10:06:23      Overcomplicated Process   \n",
       "1                    P002  2025-04-04T10:06:25         Sneaking Bad Default   \n",
       "2                    P002  2025-04-04T10:06:29          Toying With Emotion   \n",
       "3                    P002  2025-04-04T10:06:35                  Endlessness   \n",
       "4                    P003  2025-04-04T10:06:26  Expectation Result Mismatch   \n",
       "\n",
       "  metadata_interface_id  score_inefficient_efficient  \\\n",
       "0         interface_002                            6   \n",
       "1         interface_004                            3   \n",
       "2         interface_008                            2   \n",
       "3         interface_014                            5   \n",
       "4         interface_005                            4   \n",
       "\n",
       "   score_interesting_not_interesting  score_clear_confusing  \\\n",
       "0                                  5                      3   \n",
       "1                                  4                      4   \n",
       "2                                  6                      7   \n",
       "3                                  2                      5   \n",
       "4                                  4                      4   \n",
       "\n",
       "   score_enjoyable_annoying  score_organized_cluttered  \\\n",
       "0                         2                          6   \n",
       "1                         1                          6   \n",
       "2                         3                          4   \n",
       "3                         1                          6   \n",
       "4                         4                          6   \n",
       "\n",
       "   score_addictive_non_addictive  score_supportive_obstructive  \\\n",
       "0                              4                             6   \n",
       "1                              3                             4   \n",
       "2                              7                             5   \n",
       "3                              4                             3   \n",
       "4                              4                             4   \n",
       "\n",
       "   score_pressuring_suggesting  score_boring_exciting  score_revealed_covert  \\\n",
       "0                            6                      4                      5   \n",
       "1                            5                      3                      3   \n",
       "2                            2                      6                      2   \n",
       "3                            4                      7                      2   \n",
       "4                            4                      1                      4   \n",
       "\n",
       "   score_complicated_easy  score_unpredictable_predictable  \\\n",
       "0                       5                                3   \n",
       "1                       6                                4   \n",
       "2                       6                                1   \n",
       "3                       5                                4   \n",
       "4                       3                                4   \n",
       "\n",
       "   score_friendly_unfriendly  score_deceptive_benevolent data_source  \n",
       "0                          3                           5       Human  \n",
       "1                          1                           4       Human  \n",
       "2                          3                           4       Human  \n",
       "3                          2                           3       Human  \n",
       "4                          2                           4       Human  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load human data\n",
    "human_data = pd.read_csv(\"Formatting Human Survey Data/raw_participant_evaluations.csv\")\n",
    "\n",
    "# Fix column name for consistency\n",
    "if 'score_addictive_non-addictive' in human_data.columns:\n",
    "    human_data = human_data.rename(columns={'score_addictive_non-addictive': 'score_addictive_non_addictive'})\n",
    "\n",
    "# Add data source column\n",
    "human_data['data_source'] = 'Human'\n",
    "\n",
    "print(f\"Human data shape: {human_data.shape}\")\n",
    "human_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 17 model result files:\n",
      "- Found 6 Qwen files\n",
      "- Found 4 OpenAI files\n",
      "- Found 7 Anthropic files\n",
      "- Found 0 Other files\n",
      "Loaded 5 rows from results/results_openai_gpt_4_turbo_temp0.7_run1_20250404_110057.csv - Service: openai, Model: gpt-4-turbo\n",
      "Loaded 15 rows from results/results_anthropic_claude_3_opus_20240229_temp0.0_run1_20250404_114910.csv - Service: anthropic, Model: claude-3-opus-20240229\n",
      "File results/results_qwen_qwen_vl_max_temp0.0_run1_20250404_110057.csv has statistical headers, skipping first row\n",
      "Skipping results/results_qwen_qwen_vl_max_temp0.0_run1_20250404_110057.csv - missing metadata_pattern_type column\n",
      "File results/results_qwen_qwen_vl_max_temp0.0_run3_20250404_110057.csv has statistical headers, skipping first row\n",
      "Skipping results/results_qwen_qwen_vl_max_temp0.0_run3_20250404_110057.csv - missing metadata_pattern_type column\n",
      "File results/results_qwen_qwen_vl_max_temp0.0_run2_20250404_110057.csv has statistical headers, skipping first row\n",
      "Skipping results/results_qwen_qwen_vl_max_temp0.0_run2_20250404_110057.csv - missing metadata_pattern_type column\n",
      "Loaded 15 rows from results/results_anthropic_claude_3_opus_20240229_temp0.0_run2_20250404_110057.csv - Service: anthropic, Model: claude-3-opus-20240229\n",
      "Loaded 2 rows from results/results_anthropic_claude_3_opus_20240229_temp0.0_run3_20250404_110057.csv - Service: anthropic, Model: claude-3-opus-20240229\n",
      "Loaded 15 rows from results/results_anthropic_claude_3_opus_20240229_temp0.0_run1_20250404_110057.csv - Service: anthropic, Model: claude-3-opus-20240229\n",
      "File results/results_qwen_qwen_vl_max_temp0.7_run2_20250404_110057.csv has statistical headers, skipping first row\n",
      "Skipping results/results_qwen_qwen_vl_max_temp0.7_run2_20250404_110057.csv - missing metadata_pattern_type column\n",
      "File results/results_qwen_qwen_vl_max_temp0.7_run3_20250404_110057.csv has statistical headers, skipping first row\n",
      "Skipping results/results_qwen_qwen_vl_max_temp0.7_run3_20250404_110057.csv - missing metadata_pattern_type column\n",
      "File results/results_qwen_qwen_vl_max_temp0.7_run1_20250404_110057.csv has statistical headers, skipping first row\n",
      "Skipping results/results_qwen_qwen_vl_max_temp0.7_run1_20250404_110057.csv - missing metadata_pattern_type column\n",
      "Loaded 15 rows from results/results_openai_gpt_4_turbo_temp0.0_run3_20250404_110057.csv - Service: openai, Model: gpt-4-turbo\n",
      "Loaded 15 rows from results/results_openai_gpt_4_turbo_temp0.0_run2_20250404_110057.csv - Service: openai, Model: gpt-4-turbo\n",
      "Loaded 15 rows from results/results_openai_gpt_4_turbo_temp0.0_run1_20250404_110057.csv - Service: openai, Model: gpt-4-turbo\n",
      "Loaded 15 rows from results/results_anthropic_claude_3_opus_20240229_temp0.7_run1_20250404_115713.csv - Service: anthropic, Model: claude-3-opus-20240229\n",
      "Loaded 15 rows from results/results_anthropic_claude_3_opus_20240229_temp0.7_run3_20250404_115713.csv - Service: anthropic, Model: claude-3-opus-20240229\n",
      "Loaded 15 rows from results/results_anthropic_claude_3_opus_20240229_temp0.7_run2_20250404_115713.csv - Service: anthropic, Model: claude-3-opus-20240229\n",
      "\n",
      "Models loaded:\n",
      "     Service                   Model  Count\n",
      "0  anthropic  claude-3-opus-20240229     92\n",
      "1     openai             gpt-4-turbo     50\n",
      "\n",
      "Model data shape: (142, 22)\n",
      "Sample of loaded data for each service:\n",
      "\n",
      "openai - 50 rows:\n",
      "  metadata_ai_service metadata_model    metadata_pattern_type\n",
      "0              openai    gpt-4-turbo                  Nagging\n",
      "1              openai    gpt-4-turbo  Overcomplicated Process\n",
      "\n",
      "anthropic - 92 rows:\n",
      "  metadata_ai_service          metadata_model    metadata_pattern_type\n",
      "5           anthropic  claude-3-opus-20240229                  Nagging\n",
      "6           anthropic  claude-3-opus-20240229  Overcomplicated Process\n"
     ]
    }
   ],
   "source": [
    "# Load model data (combining all model results)\n",
    "model_files = [\n",
    "    f for f in os.listdir(\"results\") \n",
    "    if f.endswith('.csv') and not f.endswith('_log.csv') and f != 'api_usage.csv'\n",
    "]\n",
    "model_files = [os.path.join(\"results\", f) for f in model_files]\n",
    "\n",
    "# Print the files we're processing\n",
    "print(f\"Processing {len(model_files)} model result files:\")\n",
    "\n",
    "# Group files by model type for better analysis\n",
    "qwen_files = [f for f in model_files if 'qwen' in f.lower()]\n",
    "openai_files = [f for f in model_files if 'openai' in f.lower() or 'gpt' in f.lower()]\n",
    "anthropic_files = [f for f in model_files if 'anthropic' in f.lower() or 'claude' in f.lower()]\n",
    "other_files = [f for f in model_files if f not in qwen_files + openai_files + anthropic_files]\n",
    "\n",
    "print(f\"- Found {len(qwen_files)} Qwen files\")\n",
    "print(f\"- Found {len(openai_files)} OpenAI files\")\n",
    "print(f\"- Found {len(anthropic_files)} Anthropic files\")\n",
    "print(f\"- Found {len(other_files)} Other files\")\n",
    "\n",
    "model_data_list = []\n",
    "for file in model_files:\n",
    "    try:\n",
    "        # First try normal loading\n",
    "        df = pd.read_csv(file)\n",
    "        # Check if first row contains statistical headers\n",
    "        if any(col in str(df.iloc[0]).lower() for col in ['mean', 'std', 'min', 'max']):\n",
    "            print(f\"File {file} has statistical headers, skipping first row\")\n",
    "            df = pd.read_csv(file, skiprows=1)\n",
    "        \n",
    "        # For Qwen files, force the correct values\n",
    "        if 'qwen' in file.lower():\n",
    "            df['metadata_ai_service'] = 'qwen'\n",
    "            df['metadata_model'] = 'qwen-vl-max'\n",
    "        elif 'openai' in file.lower() or 'gpt' in file.lower():\n",
    "            df['metadata_ai_service'] = 'openai'\n",
    "            df['metadata_model'] = 'gpt-4-turbo'  \n",
    "        elif 'anthropic' in file.lower() or 'claude' in file.lower():\n",
    "            df['metadata_ai_service'] = 'anthropic'\n",
    "            df['metadata_model'] = 'claude-3-opus-20240229'\n",
    "            \n",
    "        # Check if necessary columns exist\n",
    "        if 'metadata_pattern_type' not in df.columns:\n",
    "            print(f\"Skipping {file} - missing metadata_pattern_type column\")\n",
    "            continue\n",
    "            \n",
    "        # Keep only essential columns to avoid timestamp columns that can't be averaged\n",
    "        essential_cols = [col for col in df.columns if (\n",
    "            col.startswith('metadata_') or \n",
    "            col.startswith('score_') or\n",
    "            col == 'explanation'\n",
    "        )]\n",
    "        \n",
    "        # Skip files without necessary columns\n",
    "        if not any(col.startswith('score_') for col in essential_cols):\n",
    "            print(f\"Skipping {file} - missing score columns\")\n",
    "            continue\n",
    "            \n",
    "        df = df[essential_cols]\n",
    "        \n",
    "        # Ensure service and model columns exist\n",
    "        if 'metadata_ai_service' not in df.columns:\n",
    "            print(f\"Adding missing metadata_ai_service to {file}\")\n",
    "            if 'qwen' in file.lower():\n",
    "                df['metadata_ai_service'] = 'qwen'\n",
    "            elif 'openai' in file.lower() or 'gpt' in file.lower():\n",
    "                df['metadata_ai_service'] = 'openai'\n",
    "            elif 'anthropic' in file.lower() or 'claude' in file.lower():\n",
    "                df['metadata_ai_service'] = 'anthropic'\n",
    "                \n",
    "        if 'metadata_model' not in df.columns:\n",
    "            print(f\"Adding missing metadata_model to {file}\")\n",
    "            if 'qwen' in file.lower():\n",
    "                df['metadata_model'] = 'qwen-vl-max'\n",
    "            elif 'gpt-4' in file.lower():\n",
    "                df['metadata_model'] = 'gpt-4-turbo'\n",
    "            elif 'claude' in file.lower():\n",
    "                df['metadata_model'] = 'claude-3-opus-20240229'\n",
    "        \n",
    "        # Ensure consistent data types for score columns\n",
    "        for col in df.columns:\n",
    "            if col.startswith('score_'):\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Verify we have the core columns we need\n",
    "        if 'metadata_pattern_type' in df.columns and 'metadata_ai_service' in df.columns and 'metadata_model' in df.columns:\n",
    "            # Print a sample of the models in this file\n",
    "            sample_service = df['metadata_ai_service'].iloc[0] if not df.empty else \"unknown\"\n",
    "            sample_model = df['metadata_model'].iloc[0] if not df.empty else \"unknown\"\n",
    "            print(f\"Loaded {len(df)} rows from {file} - Service: {sample_service}, Model: {sample_model}\")\n",
    "            model_data_list.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping {file} - missing required metadata columns\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Combine all model data\n",
    "if model_data_list:\n",
    "    model_data = pd.concat(model_data_list, ignore_index=True)\n",
    "    \n",
    "    # Fix column name for consistency\n",
    "    if 'score_addictive_non-addictive' in model_data.columns:\n",
    "        model_data = model_data.rename(columns={'score_addictive_non-addictive': 'score_addictive_non_addictive'})\n",
    "    \n",
    "    # Add data source column\n",
    "    model_data['data_source'] = 'AI Model'\n",
    "    \n",
    "    # Summary of loaded models\n",
    "    model_summary = model_data.groupby(['metadata_ai_service', 'metadata_model']).size().reset_index()\n",
    "    model_summary.columns = ['Service', 'Model', 'Count']\n",
    "    print(\"\\nModels loaded:\")\n",
    "    print(model_summary)\n",
    "    \n",
    "    print(f\"\\nModel data shape: {model_data.shape}\")\n",
    "    print(\"Sample of loaded data for each service:\")\n",
    "    for service in model_data['metadata_ai_service'].unique():\n",
    "        sample = model_data[model_data['metadata_ai_service'] == service]\n",
    "        print(f\"\\n{service} - {len(sample)} rows:\")\n",
    "        print(sample[['metadata_ai_service', 'metadata_model', 'metadata_pattern_type']].head(2))\n",
    "else:\n",
    "    print(\"No valid model data files found!\")\n",
    "    model_data = pd.DataFrame()  # Create empty DataFrame to avoid errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate UX KPI\n",
    "\n",
    "Calculate the UX KPI for both human and model assessments based on negative UX aspects."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def calculate_ux_kpi(df):\n    \"\"\"Calculate UX KPI based on negative UX aspects with normalized -3 to +3 scale.\"\"\"\n    # Create a copy to avoid modifying the input\n    result_df = df.copy()\n    \n    # Fix the addictive column name for consistency\n    if 'score_addictive_non-addictive' in result_df.columns and 'score_addictive_non_addictive' not in result_df.columns:\n        result_df['score_addictive_non_addictive'] = result_df['score_addictive_non-addictive']\n    \n    # Define UX KPI components\n    ux_kpi_items = {\n        'boring': 'score_boring_exciting',           # Low = boring\n        'not_interesting': 'score_interesting_not_interesting',  # High = not interesting\n        'complicated': 'score_complicated_easy',     # Low = complicated\n        'confusing': 'score_clear_confusing',        # High = confusing\n        'inefficient': 'score_inefficient_efficient', # Low = inefficient\n        'cluttered': 'score_organized_cluttered',     # High = cluttered\n        'unpredictable': 'score_unpredictable_predictable', # Low = unpredictable\n        'obstructive': 'score_supportive_obstructive'  # High = obstructive\n    }\n    \n    # Define all UEQ scales for worst aspect detection with their directions\n    all_ueq_items = {\n        # UX KPI items\n        'boring': 'score_boring_exciting',           # Low = boring\n        'not_interesting': 'score_interesting_not_interesting',  # High = not interesting\n        'complicated': 'score_complicated_easy',     # Low = complicated\n        'confusing': 'score_clear_confusing',        # High = confusing\n        'inefficient': 'score_inefficient_efficient', # Low = inefficient\n        'cluttered': 'score_organized_cluttered',     # High = cluttered\n        'unpredictable': 'score_unpredictable_predictable', # Low = unpredictable\n        'obstructive': 'score_supportive_obstructive',  # High = obstructive\n        \n        # Additional UEQ scales\n        'annoying': 'score_enjoyable_annoying',      # High = annoying\n        'pressuring': 'score_pressuring_suggesting', # Low = pressuring\n        'covert': 'score_revealed_covert',           # High = covert\n        'unfriendly': 'score_friendly_unfriendly',   # High = unfriendly\n        'deceptive': 'score_deceptive_benevolent',   # Low = deceptive\n        'addictive': 'score_addictive_non_addictive' # Low = addictive\n    }\n    \n    # Define which items need to be inverted (where higher score = better UX)\n    needs_inversion = [\n        'boring', 'complicated', 'inefficient', 'unpredictable', \n        'pressuring', 'deceptive', 'addictive'\n    ]\n    \n    # First, create normalized scores (-3 to +3 scale instead of 1-7)\n    for ueq_item, column in all_ueq_items.items():\n        if column in result_df.columns:\n            # Convert 1-7 scale to -3 to +3 scale where 0 is neutral\n            result_df[f'norm_{column}'] = result_df[column] - 4\n    \n    # Create standardized values (higher = worse UX) on the -3 to +3 scale\n    # First handle UX KPI items for UX KPI calculation\n    for ux_item, column in ux_kpi_items.items():\n        if column in result_df.columns:\n            norm_col = f'norm_{column}'\n            if ux_item in needs_inversion:\n                # These need to be inverted so high values = negative aspect\n                result_df[f'ux_{ux_item}'] = -result_df[norm_col]  # invert sign\n            else:\n                # These are already oriented so high values = negative aspect\n                result_df[f'ux_{ux_item}'] = result_df[norm_col]\n    \n    # Calculate UX KPI (mean of KPI items only)\n    ux_items = [f'ux_{item}' for item in ux_kpi_items.keys() if f'ux_{item}' in result_df.columns]\n    \n    if ux_items:\n        result_df['ux_kpi'] = result_df[ux_items].mean(axis=1)\n    \n    # Now create standardized values for ALL UEQ scales for worst aspect detection\n    for ueq_item, column in all_ueq_items.items():\n        if column in result_df.columns:\n            norm_col = f'norm_{column}'\n            # Check if this item needs to be inverted\n            if ueq_item in needs_inversion:\n                # These need to be inverted so high values = negative aspect\n                result_df[f'std_{ueq_item}'] = -result_df[norm_col]  # invert sign\n            else:\n                # These are already oriented so high values = negative aspect\n                result_df[f'std_{ueq_item}'] = result_df[norm_col]\n    \n    # Determine worst aspect from ALL UEQ scales\n    std_items = [f'std_{item}' for item in all_ueq_items.keys() if f'std_{item}' in result_df.columns]\n    \n    if std_items:\n        # Find the worst aspect (highest value is worst)\n        result_df['worst_aspect'] = result_df[std_items].idxmax(axis=1).str.replace('std_', '')\n        result_df['worst_value'] = result_df[std_items].max(axis=1)\n    \n    return result_df\n\n# Calculate UX KPI for human data\nhuman_data_with_kpi = calculate_ux_kpi(human_data)\n\n# Calculate UX KPI for model data\nmodel_data_with_kpi = calculate_ux_kpi(model_data)\n\n# Show sample results\nprint(\"Human Data with UX KPI (normalized -3 to +3 scale):\")\nprint(human_data_with_kpi[['metadata_pattern_type', 'ux_kpi', 'worst_aspect', 'worst_value']].head())\n\nprint(\"\\nModel Data with UX KPI (normalized -3 to +3 scale):\")\nprint(model_data_with_kpi[['metadata_pattern_type', 'metadata_ai_service', 'metadata_model', 'ux_kpi', 'worst_aspect', 'worst_value']].head())\n\n# Double-check direction consistency between human and model data\nprint(\"\\nDirection check - standardized values for first human entry (should be between -3 and +3):\")\nstd_cols = [col for col in human_data_with_kpi.columns if col.startswith('std_')]\nprint(human_data_with_kpi.iloc[0][std_cols])\n\nprint(\"\\nDirection check - standardized values for first model entry (should be between -3 and +3):\")\nstd_cols = [col for col in model_data_with_kpi.columns if col.startswith('std_')]\nprint(model_data_with_kpi.iloc[0][std_cols])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Human and Model Assessments\n",
    "\n",
    "Now let's generate a comparison of the average UX KPI for each pattern type between humans and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in model_pattern_kpi:\n",
      "  metadata_ai_service          metadata_model\n",
      "0           anthropic  claude-3-opus-20240229\n",
      "1              openai             gpt-4-turbo\n",
      "Adding openai gpt-4-turbo data to comparison\n",
      "  Found 15 patterns for openai gpt-4-turbo\n",
      "Adding anthropic claude-3-opus-20240229 data to comparison\n",
      "  Found 15 patterns for anthropic claude-3-opus-20240229\n",
      "\n",
      "UX KPI Comparison (Higher values = Worse UX):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata_pattern_type</th>\n",
       "      <th>human_ux_kpi</th>\n",
       "      <th>openai_gpt-4-turbo</th>\n",
       "      <th>anthropic_claude-3-opus-20240229</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nagging</td>\n",
       "      <td>4.861842</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>4.053571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False Hierarchy</td>\n",
       "      <td>4.704545</td>\n",
       "      <td>3.541667</td>\n",
       "      <td>3.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Toying With Emotion</td>\n",
       "      <td>4.672619</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>3.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Expectation Result Mismatch</td>\n",
       "      <td>4.475962</td>\n",
       "      <td>4.218750</td>\n",
       "      <td>5.270833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Trick Wording</td>\n",
       "      <td>4.431818</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sneaking Bad Default</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Social Pressure</td>\n",
       "      <td>4.310185</td>\n",
       "      <td>2.958333</td>\n",
       "      <td>2.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forced Access</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>4.208333</td>\n",
       "      <td>4.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Overcomplicated Process</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>4.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Social Connector</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hindering Account Deletion</td>\n",
       "      <td>4.173913</td>\n",
       "      <td>3.468750</td>\n",
       "      <td>4.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Content Customization</td>\n",
       "      <td>4.119565</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>4.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gamification</td>\n",
       "      <td>4.111842</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Endlessness</td>\n",
       "      <td>3.910714</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>3.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pull To Refresh</td>\n",
       "      <td>3.804348</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>4.020833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metadata_pattern_type  human_ux_kpi  openai_gpt-4-turbo  \\\n",
       "7                       Nagging      4.861842            3.843750   \n",
       "3               False Hierarchy      4.704545            3.541667   \n",
       "13          Toying With Emotion      4.672619            3.875000   \n",
       "2   Expectation Result Mismatch      4.475962            4.218750   \n",
       "14                Trick Wording      4.431818            4.333333   \n",
       "10         Sneaking Bad Default      4.388889            3.718750   \n",
       "12              Social Pressure      4.310185            2.958333   \n",
       "4                 Forced Access      4.290000            4.208333   \n",
       "8       Overcomplicated Process      4.285714            3.718750   \n",
       "11             Social Connector      4.187500            3.416667   \n",
       "6    Hindering Account Deletion      4.173913            3.468750   \n",
       "0         Content Customization      4.119565            3.416667   \n",
       "5                  Gamification      4.111842            3.000000   \n",
       "1                   Endlessness      3.910714            3.125000   \n",
       "9               Pull To Refresh      3.804348            3.875000   \n",
       "\n",
       "    anthropic_claude-3-opus-20240229  \n",
       "7                           4.053571  \n",
       "3                           3.479167  \n",
       "13                          3.541667  \n",
       "2                           5.270833  \n",
       "14                          4.979167  \n",
       "10                          3.333333  \n",
       "12                          2.833333  \n",
       "4                           4.854167  \n",
       "8                           4.857143  \n",
       "11                          2.916667  \n",
       "6                           4.437500  \n",
       "0                           4.083333  \n",
       "5                           3.000000  \n",
       "1                           3.458333  \n",
       "9                           4.020833  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average UX KPI for each pattern type - Human data\n",
    "human_pattern_kpi = human_data_with_kpi.groupby('metadata_pattern_type')['ux_kpi'].mean().reset_index()\n",
    "human_pattern_kpi = human_pattern_kpi.rename(columns={'ux_kpi': 'human_ux_kpi'})\n",
    "\n",
    "# Fix the \"Pull to Refresh\" and \"Pull To Refresh\" inconsistency\n",
    "human_pattern_kpi['metadata_pattern_type'] = human_pattern_kpi['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "\n",
    "# Calculate average UX KPI for each pattern type and model\n",
    "model_pattern_kpi = model_data_with_kpi.groupby(['metadata_pattern_type', 'metadata_ai_service', 'metadata_model'])['ux_kpi'].mean().reset_index()\n",
    "# Fix pattern names in model data\n",
    "model_pattern_kpi['metadata_pattern_type'] = model_pattern_kpi['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "\n",
    "# Print summary of models in the model pattern KPI data\n",
    "print(\"Models in model_pattern_kpi:\")\n",
    "print(model_pattern_kpi[['metadata_ai_service', 'metadata_model']].drop_duplicates())\n",
    "\n",
    "# Create separate columns for each model\n",
    "model_comparison_data = human_pattern_kpi.copy()\n",
    "\n",
    "# Add each model's data as a separate column\n",
    "for service in model_data_with_kpi['metadata_ai_service'].unique():\n",
    "    if pd.isna(service):\n",
    "        continue\n",
    "        \n",
    "    for model in model_data_with_kpi[model_data_with_kpi['metadata_ai_service'] == service]['metadata_model'].unique():\n",
    "        if pd.isna(model):\n",
    "            continue\n",
    "            \n",
    "        # Print for debugging\n",
    "        print(f\"Adding {service} {model} data to comparison\")\n",
    "            \n",
    "        # Filter for this service/model\n",
    "        filter_mask = (model_pattern_kpi['metadata_ai_service'] == service) & (model_pattern_kpi['metadata_model'] == model)\n",
    "        model_data_subset = model_pattern_kpi[filter_mask]\n",
    "        \n",
    "        # Skip if no data for this model\n",
    "        if len(model_data_subset) == 0:\n",
    "            print(f\"No data for {service} {model}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Print number of patterns for this model\n",
    "        print(f\"  Found {len(model_data_subset)} patterns for {service} {model}\")\n",
    "        \n",
    "        # Rename column and merge\n",
    "        model_data_subset = model_data_subset.rename(columns={'ux_kpi': f'{service}_{model}'})\n",
    "        model_comparison_data = model_comparison_data.merge(\n",
    "            model_data_subset[['metadata_pattern_type', f'{service}_{model}']],\n",
    "            on='metadata_pattern_type',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "# Sort by human UX KPI (worst to best)\n",
    "combined_kpi = model_comparison_data.sort_values('human_ux_kpi', ascending=False)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nUX KPI Comparison (Higher values = Worse UX):\")\n",
    "combined_kpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Comparison\n",
    "\n",
    "Let's create a bar chart to compare human vs. model average UX KPI for each pattern type."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for visualization\n# Create a long-format dataframe for easier plotting\ncomparison_data = []\n\n# Add human data\nfor idx, row in human_pattern_kpi.iterrows():\n    pattern = row['metadata_pattern_type']\n    comparison_data.append({\n        'pattern_type': pattern,\n        'source': 'Human',\n        'model': 'Human',\n        'ux_kpi': row['human_ux_kpi']\n    })\n\n# Print out model_pattern_kpi info for debugging\nprint(\"Model Pattern KPI data:\")\nprint(f\"Shape: {model_pattern_kpi.shape}\")\nprint(\"Services and models:\")\nprint(model_pattern_kpi[['metadata_ai_service', 'metadata_model']].drop_duplicates())\n\n# Add model data directly from model_pattern_kpi\nfor idx, row in model_pattern_kpi.iterrows():\n    pattern = row['metadata_pattern_type']\n    service = row['metadata_ai_service']\n    model = row['metadata_model']\n    \n    comparison_data.append({\n        'pattern_type': pattern,\n        'source': service,\n        'model': model,\n        'ux_kpi': row['ux_kpi']\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\n\n# Count patterns by source for verification\nprint(\"\\nPattern counts by source:\")\nsource_counts = comparison_df.groupby('source')['pattern_type'].count()\nprint(source_counts)\n\n# Fix the \"Pull to Refresh\" and \"Pull To Refresh\" inconsistency\ncomparison_df['pattern_type'] = comparison_df['pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n\n# Create bar chart with specific color palette for each model\nplt.figure(figsize=(16, 10))\n\n# Set up a clear color palette\npalette = {'Human': 'royalblue', 'openai': 'darkorange', 'anthropic': 'forestgreen', 'qwen': 'darkviolet'}\n\n# Create the bar plot\nax = sns.barplot(x='pattern_type', y='ux_kpi', hue='source', data=comparison_df, palette=palette)\n\n# Add a horizontal line at y=0 (neutral)\nplt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n\n# Customize chart\nplt.title('UX KPI Comparison: Human vs. AI Models (-3 to +3 scale)', fontsize=16)\nplt.xlabel('Pattern Type', fontsize=14)\nplt.ylabel('UX KPI (Higher = Worse UX, 0 = Neutral)', fontsize=14)\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Source', fontsize=12)\n\n# Set y-axis limits to show the full -3 to +3 range\nplt.ylim(-3.5, 3.5)  \n\n# Add grid for better readability\nplt.grid(axis='y', linestyle='--', alpha=0.3)\n\nplt.tight_layout()\n\n# Save figure\nplt.savefig(os.path.join(output_dir, 'human_model_ux_kpi_comparison_normalized.png'))\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gauge Visualizations\n",
    "\n",
    "Now let's create gauge visualizations for both human and model assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 worst aspects for each pattern (from all UEQ scales):\n",
      "Content Customization: cluttered (4.52), not_interesting (4.26), complicated (4.26)\n",
      "Endlessness: cluttered (5.14), obstructive (4.86), deceptive (4.52)\n",
      "Expectation Result Mismatch: unpredictable (5.15), pressuring (5.15), complicated (5.04)\n",
      "False Hierarchy: unpredictable (6.27), inefficient (6.05), confusing (5.23)\n",
      "Forced Access: inefficient (5.28), unpredictable (5.20), confusing (4.80)\n",
      "Gamification: complicated (4.68), inefficient (4.47), unpredictable (4.32)\n",
      "Hindering Account Deletion: not_interesting (4.52), obstructive (4.39), cluttered (4.35)\n",
      "Nagging: inefficient (6.00), confusing (5.84), annoying (5.68)\n",
      "Overcomplicated Process: complicated (5.14), pressuring (4.93), unpredictable (4.86)\n",
      "Pull To Refresh: cluttered (5.39), obstructive (4.70), covert (4.26)\n",
      "Sneaking Bad Default: boring (5.15), cluttered (4.74), inefficient (4.67)\n",
      "Social Connector: obstructive (4.82), cluttered (4.73), unpredictable (4.41)\n",
      "Social Pressure: cluttered (5.67), not_interesting (4.67), annoying (4.67)\n",
      "Toying With Emotion: inefficient (5.81), unpredictable (5.67), confusing (5.52)\n",
      "Trick Wording: boring (5.50), unpredictable (5.36), inefficient (5.18)\n",
      "Human pattern metrics saved to temp/human_pattern_metrics.csv\n",
      "Human pattern UEQ scores saved to temp/human_pattern_all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Calculate UX metrics by pattern for human data\n",
    "human_pattern_avg = human_data_with_kpi.groupby('metadata_pattern_type').agg({\n",
    "    'ux_kpi': 'mean',\n",
    "    'worst_aspect': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
    "    'worst_value': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Fix the \"Pull to Refresh\" and \"Pull To Refresh\" inconsistency\n",
    "human_pattern_avg['metadata_pattern_type'] = human_pattern_avg['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "\n",
    "# Show all UEQ scores for each pattern\n",
    "pattern_scores = {}\n",
    "\n",
    "# Get all standardized metric columns (std_*)\n",
    "std_cols = [col for col in human_data_with_kpi.columns if col.startswith('std_')]\n",
    "\n",
    "# Calculate mean scores for each metric by pattern type\n",
    "for pattern, group in human_data_with_kpi.groupby('metadata_pattern_type'):\n",
    "    # Calculate mean for each standardized UEQ metric\n",
    "    metric_means = {}\n",
    "    for col in std_cols:\n",
    "        metric = col.replace('std_', '')\n",
    "        metric_means[metric] = group[col].mean()\n",
    "    \n",
    "    pattern_scores[pattern] = metric_means\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "pattern_scores_df = pd.DataFrame.from_dict(pattern_scores, orient='index')\n",
    "\n",
    "# Sort metrics from worst to best for each pattern\n",
    "pattern_ranking = {}\n",
    "for pattern in pattern_scores:\n",
    "    # Sort metrics by score (higher = worse)\n",
    "    sorted_metrics = sorted(pattern_scores[pattern].items(), key=lambda x: x[1], reverse=True)\n",
    "    pattern_ranking[pattern] = sorted_metrics\n",
    "\n",
    "# Print worst aspects for each pattern\n",
    "print(\"Top 3 worst aspects for each pattern (from all UEQ scales):\")\n",
    "for pattern, metrics in pattern_ranking.items():\n",
    "    if len(metrics) >= 3:\n",
    "        print(f\"{pattern}: {metrics[0][0]} ({metrics[0][1]:.2f}), {metrics[1][0]} ({metrics[1][1]:.2f}), {metrics[2][0]} ({metrics[2][1]:.2f})\")\n",
    "\n",
    "# Save human pattern metrics to CSV \n",
    "human_metrics_path = os.path.join(output_dir, 'human_pattern_metrics.csv')\n",
    "human_pattern_avg.to_csv(human_metrics_path, index=False)\n",
    "\n",
    "# Save full UEQ scores by pattern\n",
    "pattern_scores_df.to_csv(os.path.join(output_dir, 'human_pattern_all_metrics.csv'))\n",
    "print(f\"Human pattern metrics saved to {human_metrics_path}\")\n",
    "print(f\"Human pattern UEQ scores saved to {os.path.join(output_dir, 'human_pattern_all_metrics.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "openai gpt-4-turbo worst aspects for Content Customization:\n",
      "  unfriendly: 6.00\n",
      "  annoying: 5.00\n",
      "  obstructive: 4.67\n",
      "openai gpt-4-turbo pattern metrics saved to temp/openai_gpt-4-turbo_pattern_metrics.csv\n",
      "openai gpt-4-turbo detailed UEQ metrics saved to temp/openai_gpt-4-turbo_pattern_all_metrics.csv\n",
      "\n",
      "anthropic claude-3-opus-20240229 worst aspects for Content Customization:\n",
      "  addictive: 6.00\n",
      "  pressuring: 5.67\n",
      "  confusing: 5.50\n",
      "anthropic claude-3-opus-20240229 pattern metrics saved to temp/anthropic_claude-3-opus-20240229_pattern_metrics.csv\n",
      "anthropic claude-3-opus-20240229 detailed UEQ metrics saved to temp/anthropic_claude-3-opus-20240229_pattern_all_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/73cb2jcd6jg68q_rlbh1rhzw0000gp/T/ipykernel_91692/3949595329.py:12: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/66/73cb2jcd6jg68q_rlbh1rhzw0000gp/T/ipykernel_91692/3949595329.py:12: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate model pattern metrics\n",
    "model_pattern_metrics = {}\n",
    "\n",
    "for service in model_data_with_kpi['metadata_ai_service'].unique():\n",
    "    for model in model_data_with_kpi[model_data_with_kpi['metadata_ai_service'] == service]['metadata_model'].unique():\n",
    "        # Filter data for this model\n",
    "        model_filter = (model_data_with_kpi['metadata_ai_service'] == service) & \\\n",
    "                       (model_data_with_kpi['metadata_model'] == model)\n",
    "        this_model_data = model_data_with_kpi[model_filter]\n",
    "        \n",
    "        # Normalize pattern names\n",
    "        this_model_data['metadata_pattern_type'] = this_model_data['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "        \n",
    "        # Calculate average ux values, worst aspect, and UX KPI for each pattern type\n",
    "        model_pattern_avg = this_model_data.groupby('metadata_pattern_type').agg({\n",
    "            'ux_kpi': 'mean',\n",
    "            'worst_aspect': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
    "            'worst_value': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Save to dictionary\n",
    "        model_pattern_metrics[f\"{service}_{model}\"] = model_pattern_avg\n",
    "        \n",
    "        # Get all standardized metric columns (std_*)\n",
    "        std_cols = [col for col in this_model_data.columns if col.startswith('std_')]\n",
    "        \n",
    "        # Calculate and show detailed UEQ metrics\n",
    "        pattern_scores = {}\n",
    "        \n",
    "        for pattern, group in this_model_data.groupby('metadata_pattern_type'):\n",
    "            # Calculate mean for each standardized UEQ metric\n",
    "            metric_means = {}\n",
    "            for col in std_cols:\n",
    "                metric = col.replace('std_', '')\n",
    "                metric_means[metric] = group[col].mean()\n",
    "            \n",
    "            pattern_scores[pattern] = metric_means\n",
    "        \n",
    "        # Convert to DataFrame for easier viewing\n",
    "        pattern_scores_df = pd.DataFrame.from_dict(pattern_scores, orient='index')\n",
    "        \n",
    "        # Print worst aspects for a sample pattern\n",
    "        if len(pattern_scores) > 0:\n",
    "            sample_pattern = list(pattern_scores.keys())[0]\n",
    "            sorted_metrics = sorted(pattern_scores[sample_pattern].items(), key=lambda x: x[1], reverse=True)\n",
    "            print(f\"\\n{service} {model} worst aspects for {sample_pattern}:\")\n",
    "            for metric, value in sorted_metrics[:3]:\n",
    "                print(f\"  {metric}: {value:.2f}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        model_metrics_path = os.path.join(output_dir, f'{service}_{model}_pattern_metrics.csv')\n",
    "        model_pattern_avg.to_csv(model_metrics_path, index=False)\n",
    "        \n",
    "        # Save full UEQ scores\n",
    "        detailed_metrics_path = os.path.join(output_dir, f'{service}_{model}_pattern_all_metrics.csv')\n",
    "        pattern_scores_df.to_csv(detailed_metrics_path)\n",
    "        \n",
    "        print(f\"{service} {model} pattern metrics saved to {model_metrics_path}\")\n",
    "        print(f\"{service} {model} detailed UEQ metrics saved to {detailed_metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Worst Aspects\n",
    "\n",
    "Compare which aspects were rated as worst by humans vs. models for each pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Worst UX Aspects:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/73cb2jcd6jg68q_rlbh1rhzw0000gp/T/ipykernel_91692/1272727393.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/66/73cb2jcd6jg68q_rlbh1rhzw0000gp/T/ipykernel_91692/1272727393.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata_pattern_type</th>\n",
       "      <th>human_worst_aspect</th>\n",
       "      <th>openai_gpt-4-turbo_worst_aspect</th>\n",
       "      <th>anthropic_claude-3-opus-20240229_worst_aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Content Customization</td>\n",
       "      <td>boring</td>\n",
       "      <td>unfriendly</td>\n",
       "      <td>confusing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Endlessness</td>\n",
       "      <td>cluttered</td>\n",
       "      <td>covert</td>\n",
       "      <td>confusing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Expectation Result Mismatch</td>\n",
       "      <td>boring</td>\n",
       "      <td>boring</td>\n",
       "      <td>addictive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False Hierarchy</td>\n",
       "      <td>complicated</td>\n",
       "      <td>pressuring</td>\n",
       "      <td>not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forced Access</td>\n",
       "      <td>confusing</td>\n",
       "      <td>not_interesting</td>\n",
       "      <td>not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gamification</td>\n",
       "      <td>complicated</td>\n",
       "      <td>cluttered</td>\n",
       "      <td>pressuring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hindering Account Deletion</td>\n",
       "      <td>boring</td>\n",
       "      <td>not_interesting</td>\n",
       "      <td>addictive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nagging</td>\n",
       "      <td>boring</td>\n",
       "      <td>boring</td>\n",
       "      <td>obstructive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Overcomplicated Process</td>\n",
       "      <td>complicated</td>\n",
       "      <td>boring</td>\n",
       "      <td>addictive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pull To Refresh</td>\n",
       "      <td>cluttered</td>\n",
       "      <td>unpredictable</td>\n",
       "      <td>inefficient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sneaking Bad Default</td>\n",
       "      <td>boring</td>\n",
       "      <td>covert</td>\n",
       "      <td>pressuring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Social Connector</td>\n",
       "      <td>boring</td>\n",
       "      <td>pressuring</td>\n",
       "      <td>pressuring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Social Pressure</td>\n",
       "      <td>cluttered</td>\n",
       "      <td>pressuring</td>\n",
       "      <td>pressuring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Toying With Emotion</td>\n",
       "      <td>boring</td>\n",
       "      <td>obstructive</td>\n",
       "      <td>pressuring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Trick Wording</td>\n",
       "      <td>boring</td>\n",
       "      <td>boring</td>\n",
       "      <td>addictive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metadata_pattern_type human_worst_aspect  \\\n",
       "0         Content Customization             boring   \n",
       "1                   Endlessness          cluttered   \n",
       "2   Expectation Result Mismatch             boring   \n",
       "3               False Hierarchy        complicated   \n",
       "4                 Forced Access          confusing   \n",
       "5                  Gamification        complicated   \n",
       "6    Hindering Account Deletion             boring   \n",
       "7                       Nagging             boring   \n",
       "8       Overcomplicated Process        complicated   \n",
       "9               Pull To Refresh          cluttered   \n",
       "10         Sneaking Bad Default             boring   \n",
       "11             Social Connector             boring   \n",
       "12              Social Pressure          cluttered   \n",
       "13          Toying With Emotion             boring   \n",
       "14                Trick Wording             boring   \n",
       "\n",
       "   openai_gpt-4-turbo_worst_aspect  \\\n",
       "0                       unfriendly   \n",
       "1                           covert   \n",
       "2                           boring   \n",
       "3                       pressuring   \n",
       "4                  not_interesting   \n",
       "5                        cluttered   \n",
       "6                  not_interesting   \n",
       "7                           boring   \n",
       "8                           boring   \n",
       "9                    unpredictable   \n",
       "10                          covert   \n",
       "11                      pressuring   \n",
       "12                      pressuring   \n",
       "13                     obstructive   \n",
       "14                          boring   \n",
       "\n",
       "   anthropic_claude-3-opus-20240229_worst_aspect  \n",
       "0                                      confusing  \n",
       "1                                      confusing  \n",
       "2                                      addictive  \n",
       "3                                not_interesting  \n",
       "4                                not_interesting  \n",
       "5                                     pressuring  \n",
       "6                                      addictive  \n",
       "7                                    obstructive  \n",
       "8                                      addictive  \n",
       "9                                    inefficient  \n",
       "10                                    pressuring  \n",
       "11                                    pressuring  \n",
       "12                                    pressuring  \n",
       "13                                    pressuring  \n",
       "14                                     addictive  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a comparison table of worst aspects\n",
    "worst_aspect_comparison = human_pattern_avg[['metadata_pattern_type', 'worst_aspect']].rename(\n",
    "    columns={'worst_aspect': 'human_worst_aspect'}\n",
    ")\n",
    "\n",
    "# Fix the \"Pull to Refresh\" and \"Pull To Refresh\" inconsistency\n",
    "worst_aspect_comparison['metadata_pattern_type'] = worst_aspect_comparison['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "\n",
    "# Add model worst aspects\n",
    "for service in model_data_with_kpi['metadata_ai_service'].unique():\n",
    "    for model in model_data_with_kpi[model_data_with_kpi['metadata_ai_service'] == service]['metadata_model'].unique():\n",
    "        model_filter = (model_data_with_kpi['metadata_ai_service'] == service) & \\\n",
    "                       (model_data_with_kpi['metadata_model'] == model)\n",
    "        this_model_data = model_data_with_kpi[model_filter]\n",
    "        \n",
    "        # Normalize pattern names\n",
    "        this_model_data['metadata_pattern_type'] = this_model_data['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "        \n",
    "        # Calculate most common worst aspect for each pattern type\n",
    "        model_worst_aspects = this_model_data.groupby('metadata_pattern_type')['worst_aspect'].agg(\n",
    "            lambda x: x.mode()[0] if not x.mode().empty else None\n",
    "        ).reset_index()\n",
    "        \n",
    "        model_worst_aspects = model_worst_aspects.rename(\n",
    "            columns={'worst_aspect': f'{service}_{model}_worst_aspect'}\n",
    "        )\n",
    "        \n",
    "        # Add to comparison table\n",
    "        worst_aspect_comparison = worst_aspect_comparison.merge(\n",
    "            model_worst_aspects, on='metadata_pattern_type', how='outer'\n",
    "        )\n",
    "\n",
    "# Remove duplicate rows for Pull To Refresh\n",
    "worst_aspect_comparison = worst_aspect_comparison.drop_duplicates(subset=['metadata_pattern_type'])\n",
    "\n",
    "# Display comparison table\n",
    "print(\"Comparison of Worst UX Aspects:\")\n",
    "worst_aspect_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Agreement Between Humans and Models\n",
    "\n",
    "Calculate how closely models' assessments align with human assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating inter-annotator agreement within humans...\n",
      "Human inter-annotator agreement:\n",
      "score_inefficient_efficient: Mean κ = 0.33 (min: 0.33, max: 0.33)\n",
      "score_interesting_not_interesting: Mean κ = -0.11 (min: -0.11, max: -0.11)\n",
      "score_clear_confusing: Mean κ = 0.06 (min: 0.06, max: 0.06)\n",
      "score_enjoyable_annoying: Mean κ = 0.44 (min: 0.44, max: 0.44)\n",
      "score_organized_cluttered: Mean κ = 0.06 (min: 0.06, max: 0.06)\n",
      "score_addictive_non_addictive: Mean κ = 0.17 (min: 0.17, max: 0.17)\n",
      "score_supportive_obstructive: Mean κ = -0.11 (min: -0.11, max: -0.11)\n",
      "score_pressuring_suggesting: Mean κ = -0.11 (min: -0.11, max: -0.11)\n",
      "score_boring_exciting: Mean κ = 0.44 (min: 0.44, max: 0.44)\n",
      "score_revealed_covert: Mean κ = 0.06 (min: 0.06, max: 0.06)\n",
      "score_complicated_easy: Mean κ = 0.12 (min: 0.12, max: 0.12)\n",
      "score_unpredictable_predictable: Mean κ = 0.06 (min: 0.06, max: 0.06)\n",
      "score_friendly_unfriendly: Mean κ = 0.55 (min: 0.55, max: 0.55)\n",
      "score_deceptive_benevolent: Mean κ = 0.21 (min: 0.21, max: 0.21)\n",
      "\n",
      "Calculating agreement within openai gpt-4-turbo...\n",
      "openai gpt-4-turbo agreement:\n",
      "Error: Need at least 2 participants with sufficient data\n",
      "\n",
      "Calculating agreement within anthropic claude-3-opus-20240229...\n",
      "anthropic claude-3-opus-20240229 agreement:\n",
      "Error: Need at least 2 participants with sufficient data\n",
      "\n",
      "Calculating agreement between humans and openai gpt-4-turbo...\n",
      "Human-openai gpt-4-turbo agreement:\n",
      "score_inefficient_efficient: κ = -0.07\n",
      "score_clear_confusing: κ = -0.02\n",
      "score_enjoyable_annoying: κ = 0.12\n",
      "score_organized_cluttered: κ = 0.08\n",
      "score_addictive_non_addictive: κ = -0.17\n",
      "score_pressuring_suggesting: κ = 0.06\n",
      "score_boring_exciting: κ = 0.02\n",
      "score_revealed_covert: κ = 0.16\n",
      "score_complicated_easy: κ = 0.00\n",
      "score_unpredictable_predictable: κ = -0.08\n",
      "score_friendly_unfriendly: κ = -0.08\n",
      "score_deceptive_benevolent: κ = -0.08\n",
      "\n",
      "Calculating agreement between humans and anthropic claude-3-opus-20240229...\n",
      "Human-anthropic claude-3-opus-20240229 agreement:\n",
      "score_inefficient_efficient: κ = 0.09\n",
      "score_clear_confusing: κ = -0.10\n",
      "score_enjoyable_annoying: κ = 0.08\n",
      "score_organized_cluttered: κ = -0.08\n",
      "score_addictive_non_addictive: κ = -0.04\n",
      "score_pressuring_suggesting: κ = 0.04\n",
      "score_boring_exciting: κ = -0.22\n",
      "score_revealed_covert: κ = 0.20\n",
      "score_complicated_easy: κ = -0.05\n",
      "score_unpredictable_predictable: κ = 0.37\n",
      "score_friendly_unfriendly: κ = -0.07\n",
      "score_deceptive_benevolent: κ = 0.03\n",
      "\n",
      "Agreement results saved to temp/inter_annotator_agreement.json\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for reliability analysis\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Function to calculate inter-annotator agreement within a group\n",
    "def calculate_within_group_agreement(df, group_col, pattern_col, metric_cols):\n",
    "    \"\"\"Calculate agreement metrics within a group (humans or specific AI model)\"\"\"\n",
    "    agreement_results = {}\n",
    "    \n",
    "    # Group by pattern type\n",
    "    patterns = df[pattern_col].unique()\n",
    "    participants = df[group_col].unique()\n",
    "    \n",
    "    if len(participants) < 2:\n",
    "        return {\"error\": \"Need at least 2 participants for agreement calculations\"}\n",
    "    \n",
    "    # Get participants with enough data\n",
    "    valid_participants = []\n",
    "    for participant in participants:\n",
    "        # Check how many patterns this participant rated\n",
    "        mask = df[group_col] == participant\n",
    "        if df[mask][pattern_col].nunique() >= 5:  # Require at least 5 patterns\n",
    "            valid_participants.append(participant)\n",
    "    \n",
    "    if len(valid_participants) < 2:\n",
    "        return {\"error\": \"Need at least 2 participants with sufficient data\"}\n",
    "    \n",
    "    # For each metric\n",
    "    for metric in metric_cols:\n",
    "        # Create a matrix of ratings: rows=patterns, columns=participants\n",
    "        ratings_matrix = []\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            pattern_ratings = []\n",
    "            for participant in valid_participants:\n",
    "                # Get this participant's rating for this pattern\n",
    "                mask = (df[pattern_col] == pattern) & (df[group_col] == participant)\n",
    "                \n",
    "                # If multiple ratings exist, take the mean\n",
    "                ratings = df.loc[mask, metric].mean() if sum(mask) > 0 else None\n",
    "                pattern_ratings.append(ratings)\n",
    "            \n",
    "            # Only include patterns where at least 2 participants have rated\n",
    "            if sum(1 for r in pattern_ratings if r is not None) >= 2:\n",
    "                ratings_matrix.append(pattern_ratings)\n",
    "        \n",
    "        # Skip if not enough data\n",
    "        if len(ratings_matrix) < 5:  # Need at least 5 patterns\n",
    "            continue\n",
    "            \n",
    "        # Convert to numeric categories for kappa calculation (1-7 scale to 0-2 categories)\n",
    "        categorized_matrix = []\n",
    "        for pattern_ratings in ratings_matrix:\n",
    "            categorized_ratings = []\n",
    "            for rating in pattern_ratings:\n",
    "                if pd.isna(rating):\n",
    "                    categorized_ratings.append(None)\n",
    "                elif rating <= 3:\n",
    "                    categorized_ratings.append(0)  # Low\n",
    "                elif rating <= 5:\n",
    "                    categorized_ratings.append(1)  # Medium\n",
    "                else:\n",
    "                    categorized_ratings.append(2)  # High\n",
    "            categorized_matrix.append(categorized_ratings)\n",
    "        \n",
    "        # Calculate pairwise Cohen's kappa\n",
    "        kappa_values = []\n",
    "        \n",
    "        for i in range(len(valid_participants)):\n",
    "            for j in range(i+1, len(valid_participants)):\n",
    "                # Extract ratings for these two participants\n",
    "                ratings_i = []\n",
    "                ratings_j = []\n",
    "                \n",
    "                for pattern_idx in range(len(categorized_matrix)):\n",
    "                    if (categorized_matrix[pattern_idx][i] is not None and \n",
    "                        categorized_matrix[pattern_idx][j] is not None):\n",
    "                        ratings_i.append(categorized_matrix[pattern_idx][i])\n",
    "                        ratings_j.append(categorized_matrix[pattern_idx][j])\n",
    "                \n",
    "                # Calculate kappa if we have enough data\n",
    "                if len(ratings_i) >= 5:  # Need at least 5 common patterns\n",
    "                    try:\n",
    "                        # Check if there's enough variation in the ratings (both must have at least 2 category)\n",
    "                        if len(set(ratings_i)) >= 2 and len(set(ratings_j)) >= 2:\n",
    "                            kappa = cohen_kappa_score(ratings_i, ratings_j)\n",
    "                            kappa_values.append(kappa)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating kappa: {e}\")\n",
    "        \n",
    "        # Store results\n",
    "        if kappa_values:\n",
    "            agreement_results[metric] = {\n",
    "                'mean_kappa': sum(kappa_values) / len(kappa_values),\n",
    "                'min_kappa': min(kappa_values),\n",
    "                'max_kappa': max(kappa_values),\n",
    "                'num_pairs': len(kappa_values)\n",
    "            }\n",
    "    \n",
    "    return agreement_results\n",
    "\n",
    "# Calculate agreement within humans\n",
    "print(\"Calculating inter-annotator agreement within humans...\")\n",
    "try:\n",
    "    human_agreement = calculate_within_group_agreement(\n",
    "        human_data_with_kpi, \n",
    "        'metadata_participant_id', \n",
    "        'metadata_pattern_type', \n",
    "        [col for col in human_data_with_kpi.columns if col.startswith('score_')]\n",
    "    )\n",
    "\n",
    "    print(\"Human inter-annotator agreement:\")\n",
    "    if isinstance(human_agreement, dict) and \"error\" in human_agreement:\n",
    "        print(f\"Error: {human_agreement['error']}\")\n",
    "    else:\n",
    "        for metric, values in human_agreement.items():\n",
    "            if isinstance(values, dict) and 'mean_kappa' in values:\n",
    "                print(f\"{metric}: Mean κ = {values['mean_kappa']:.2f} (min: {values['min_kappa']:.2f}, max: {values['max_kappa']:.2f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in human agreement calculation: {e}\")\n",
    "\n",
    "# Calculate agreement within each model\n",
    "model_agreement = {}\n",
    "\n",
    "for service in model_data_with_kpi['metadata_ai_service'].unique():\n",
    "    if pd.isna(service):\n",
    "        continue\n",
    "        \n",
    "    for model in model_data_with_kpi[model_data_with_kpi['metadata_ai_service'] == service]['metadata_model'].unique():\n",
    "        if pd.isna(model):\n",
    "            continue\n",
    "            \n",
    "        # Filter to just this model's data\n",
    "        model_filter = (model_data_with_kpi['metadata_ai_service'] == service) & \\\n",
    "                      (model_data_with_kpi['metadata_model'] == model)\n",
    "        this_model_data = model_data_with_kpi[model_filter]\n",
    "        \n",
    "        # Skip if there's not enough data\n",
    "        if len(this_model_data) < 10:\n",
    "            print(f\"\\nSkipping {service} {model} - not enough data\")\n",
    "            continue\n",
    "        \n",
    "        # Check if we have multiple runs/participants\n",
    "        if len(this_model_data['metadata_timestamp'].unique()) > 1:\n",
    "            print(f\"\\nCalculating agreement within {service} {model}...\")\n",
    "            # Use timestamp as the participant ID if no participant ID column\n",
    "            participant_col = 'metadata_participant_id' if 'metadata_participant_id' in this_model_data.columns else 'metadata_timestamp'\n",
    "            \n",
    "            try:\n",
    "                model_agreement[f\"{service}_{model}\"] = calculate_within_group_agreement(\n",
    "                    this_model_data,\n",
    "                    participant_col,\n",
    "                    'metadata_pattern_type',\n",
    "                    [col for col in this_model_data.columns if col.startswith('score_')]\n",
    "                )\n",
    "                \n",
    "                print(f\"{service} {model} agreement:\")\n",
    "                if isinstance(model_agreement[f\"{service}_{model}\"], dict) and \"error\" in model_agreement[f\"{service}_{model}\"]:\n",
    "                    print(f\"Error: {model_agreement[f'{service}_{model}']['error']}\")\n",
    "                else:\n",
    "                    for metric, values in model_agreement[f\"{service}_{model}\"].items():\n",
    "                        if isinstance(values, dict) and 'mean_kappa' in values:\n",
    "                            print(f\"{metric}: Mean κ = {values['mean_kappa']:.2f} (min: {values['min_kappa']:.2f}, max: {values['max_kappa']:.2f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {service} {model} agreement calculation: {e}\")\n",
    "        else:\n",
    "            print(f\"\\nSkipping {service} {model} - not enough runs for agreement calculation\")\n",
    "\n",
    "# Calculate agreement between humans and each model\n",
    "human_model_agreement = {}\n",
    "\n",
    "# Define a helper function to safely calculate mean for each metric\n",
    "def safe_calculate_pattern_metrics(df, by_col):\n",
    "    \"\"\"Calculate means for metrics, ensuring to select only numeric columns\"\"\"\n",
    "    # First, get all numeric score columns\n",
    "    score_cols = [col for col in df.columns if col.startswith('score_')]\n",
    "    \n",
    "    # Group by pattern and calculate mean\n",
    "    try:\n",
    "        agg_dict = {col: 'mean' for col in score_cols}\n",
    "        result = df.groupby(by_col).agg(agg_dict)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in safe_calculate_pattern_metrics: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Calculate human average scores safely\n",
    "human_avg = safe_calculate_pattern_metrics(human_data_with_kpi, 'metadata_pattern_type')\n",
    "\n",
    "for service in model_data_with_kpi['metadata_ai_service'].unique():\n",
    "    if pd.isna(service):\n",
    "        continue\n",
    "        \n",
    "    for model in model_data_with_kpi[model_data_with_kpi['metadata_ai_service'] == service]['metadata_model'].unique():\n",
    "        if pd.isna(model):\n",
    "            continue\n",
    "            \n",
    "        # Filter to just this model's data\n",
    "        model_filter = (model_data_with_kpi['metadata_ai_service'] == service) & \\\n",
    "                      (model_data_with_kpi['metadata_model'] == model)\n",
    "        this_model_data = model_data_with_kpi[model_filter]\n",
    "        \n",
    "        # Skip if there's not enough data\n",
    "        if len(this_model_data) < 10:\n",
    "            print(f\"\\nSkipping {service} {model} human comparison - not enough data\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate average scores for each pattern\n",
    "        model_avg = safe_calculate_pattern_metrics(this_model_data, 'metadata_pattern_type')\n",
    "        \n",
    "        # Skip if empty\n",
    "        if model_avg.empty:\n",
    "            print(f\"\\nSkipping {service} {model} - error calculating metrics\")\n",
    "            continue\n",
    "            \n",
    "        # Find patterns in common\n",
    "        common_patterns = set(model_avg.index) & set(human_avg.index)\n",
    "        \n",
    "        if len(common_patterns) >= 5:  # Need at least 5 common patterns\n",
    "            print(f\"\\nCalculating agreement between humans and {service} {model}...\")\n",
    "            \n",
    "            # Calculate agreement for each metric\n",
    "            agreement_results = {}\n",
    "            \n",
    "            for metric in [col for col in model_avg.columns if col.startswith('score_')]:\n",
    "                if metric in human_avg.columns:\n",
    "                    try:\n",
    "                        # Get ratings for common patterns\n",
    "                        model_ratings = []\n",
    "                        human_ratings = []\n",
    "                        \n",
    "                        for pattern in common_patterns:\n",
    "                            if not pd.isna(model_avg.loc[pattern, metric]) and not pd.isna(human_avg.loc[pattern, metric]):\n",
    "                                # Categorize ratings\n",
    "                                model_cat = 0 if model_avg.loc[pattern, metric] <= 3 else 1 if model_avg.loc[pattern, metric] <= 5 else 2\n",
    "                                human_cat = 0 if human_avg.loc[pattern, metric] <= 3 else 1 if human_avg.loc[pattern, metric] <= 5 else 2\n",
    "                                \n",
    "                                model_ratings.append(model_cat)\n",
    "                                human_ratings.append(human_cat)\n",
    "                        \n",
    "                        # Calculate kappa if we have enough data and enough variation\n",
    "                        if len(model_ratings) >= 5 and len(set(model_ratings)) >= 2 and len(set(human_ratings)) >= 2:\n",
    "                            kappa = cohen_kappa_score(model_ratings, human_ratings)\n",
    "                            agreement_results[metric] = kappa\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating {metric} agreement: {e}\")\n",
    "            \n",
    "            human_model_agreement[f\"{service}_{model}\"] = agreement_results\n",
    "            \n",
    "            print(f\"Human-{service} {model} agreement:\")\n",
    "            if not agreement_results:\n",
    "                print(\"No metrics had sufficient variation for kappa calculation\")\n",
    "            else:\n",
    "                for metric, kappa in agreement_results.items():\n",
    "                    print(f\"{metric}: κ = {kappa:.2f}\")\n",
    "        else:\n",
    "            print(f\"\\nSkipping human-{service} {model} agreement - not enough common patterns ({len(common_patterns)} < 5)\")\n",
    "\n",
    "# Save all agreement results\n",
    "agreement_summary = {\n",
    "    \"within_human\": human_agreement,\n",
    "    \"within_model\": model_agreement,\n",
    "    \"human_model\": human_model_agreement\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(output_dir, 'inter_annotator_agreement.json'), 'w') as f:\n",
    "    json.dump(agreement_summary, f, indent=2, default=str)\n",
    "print(f\"\\nAgreement results saved to {os.path.join(output_dir, 'inter_annotator_agreement.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Summary Table of UX KPI By Pattern\n",
    "\n",
    "Create a clean, easy-to-read table of UX KPI scores for each pattern, comparing humans and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UX KPI Summary by Pattern Type:\n",
      "Summary saved to temp/ux_kpi_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/73cb2jcd6jg68q_rlbh1rhzw0000gp/T/ipykernel_91692/1584258176.py:22: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/66/73cb2jcd6jg68q_rlbh1rhzw0000gp/T/ipykernel_91692/1584258176.py:22: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a clean summary table\n",
    "summary_table = pd.DataFrame({\n",
    "    'Pattern Type': human_pattern_avg['metadata_pattern_type'],\n",
    "    'Human UX KPI': human_pattern_avg['ux_kpi'],\n",
    "    'Human Worst Aspect': human_pattern_avg['worst_aspect']\n",
    "})\n",
    "\n",
    "# Fix the \"Pull to Refresh\" and \"Pull To Refresh\" inconsistency\n",
    "summary_table['Pattern Type'] = summary_table['Pattern Type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "\n",
    "# Add model data\n",
    "for service in model_data_with_kpi['metadata_ai_service'].unique():\n",
    "    for model in model_data_with_kpi[model_data_with_kpi['metadata_ai_service'] == service]['metadata_model'].unique():\n",
    "        if pd.isna(model):\n",
    "            continue\n",
    "            \n",
    "        model_filter = (model_data_with_kpi['metadata_ai_service'] == service) & \\\n",
    "                      (model_data_with_kpi['metadata_model'] == model)\n",
    "        this_model_data = model_data_with_kpi[model_filter]\n",
    "        \n",
    "        # Normalize pattern names\n",
    "        this_model_data['metadata_pattern_type'] = this_model_data['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n",
    "        \n",
    "        model_pattern_avg = this_model_data.groupby('metadata_pattern_type').agg({\n",
    "            'ux_kpi': 'mean',\n",
    "            'worst_aspect': lambda x: x.mode()[0] if not x.mode().empty else None\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Skip if empty\n",
    "        if len(model_pattern_avg) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Create a temporary dataframe\n",
    "        temp_df = pd.DataFrame({\n",
    "            'Pattern Type': model_pattern_avg['metadata_pattern_type'],\n",
    "            f'{service} {model} UX KPI': model_pattern_avg['ux_kpi'],\n",
    "            f'{service} {model} Worst Aspect': model_pattern_avg['worst_aspect']\n",
    "        })\n",
    "        \n",
    "        # Merge with summary table\n",
    "        summary_table = summary_table.merge(temp_df, on='Pattern Type', how='outer')\n",
    "\n",
    "# Remove duplicate rows for Pull To Refresh\n",
    "summary_table = summary_table.drop_duplicates(subset=['Pattern Type'])\n",
    "\n",
    "# Sort by Human UX KPI (worst to best)\n",
    "summary_table = summary_table.sort_values('Human UX KPI', ascending=False)\n",
    "\n",
    "# Display summary table\n",
    "print(\"UX KPI Summary by Pattern Type:\")\n",
    "summary_table\n",
    "\n",
    "# Save summary table to CSV\n",
    "summary_path = os.path.join(output_dir, 'ux_kpi_summary.csv')\n",
    "summary_table.to_csv(summary_path, index=False)\n",
    "print(f\"Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create a normalized overall UX score comparison across all scales\nprint(\"Creating normalized overall UX score comparison...\")\n\n# Define the function to calculate normalized overall UX score\ndef calculate_normalized_overall_ux(df):\n    \"\"\"Calculate the mean of all normalized UX scores (-3 to +3 scale).\"\"\"\n    # Create a copy to avoid modifying the input\n    result_df = df.copy()\n    \n    # Fix the addictive column name for consistency\n    if 'score_addictive_non-addictive' in result_df.columns and 'score_addictive_non_addictive' not in result_df.columns:\n        result_df['score_addictive_non_addictive'] = result_df['score_addictive_non-addictive']\n    \n    # Get all score columns\n    score_cols = [col for col in result_df.columns if col.startswith('score_')]\n    \n    # Standardize all scales so higher = worse UX on -3 to +3 scale\n    needs_inversion = [\n        'score_boring_exciting',\n        'score_complicated_easy', \n        'score_inefficient_efficient', \n        'score_unpredictable_predictable',\n        'score_pressuring_suggesting',\n        'score_deceptive_benevolent',\n        'score_addictive_non_addictive'\n    ]\n    \n    # First normalize all scores to -3 to +3 scale\n    for col in score_cols:\n        result_df[f'norm_{col}'] = result_df[col] - 4  # Convert 1-7 to -3 to +3\n    \n    # Create standardized versions\n    for col in score_cols:\n        norm_col = f'norm_{col}'\n        if col in needs_inversion:\n            # These need to be inverted so high values = negative aspect\n            result_df[f'std_{col}'] = -result_df[norm_col]  # Invert sign\n        else:\n            # These are already oriented so high values = negative aspect\n            result_df[f'std_{col}'] = result_df[norm_col]\n    \n    # Calculate overall UX score (mean of all standardized scales)\n    std_cols = [f'std_{col}' for col in score_cols if f'std_{col}' in result_df.columns]\n    \n    if std_cols:\n        result_df['overall_norm_ux'] = result_df[std_cols].mean(axis=1)\n    \n    return result_df\n\n# Calculate normalized overall UX score for human data\nhuman_norm = calculate_normalized_overall_ux(human_data)\n# Calculate normalized overall UX score for model data\nmodel_norm = calculate_normalized_overall_ux(model_data)\n\n# Calculate average overall UX score for each pattern type - Human data\nhuman_pattern_norm = human_norm.groupby('metadata_pattern_type')['overall_norm_ux'].mean().reset_index()\nhuman_pattern_norm = human_pattern_norm.rename(columns={'overall_norm_ux': 'human_overall_ux'})\n\n# Fix the \"Pull to Refresh\" and \"Pull To Refresh\" inconsistency\nhuman_pattern_norm['metadata_pattern_type'] = human_pattern_norm['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n\n# Calculate average overall UX score for each pattern type and model\nmodel_pattern_norm = model_norm.groupby(['metadata_pattern_type', 'metadata_ai_service', 'metadata_model'])['overall_norm_ux'].mean().reset_index()\n# Fix pattern names in model data\nmodel_pattern_norm['metadata_pattern_type'] = model_pattern_norm['metadata_pattern_type'].replace('Pull to Refresh', 'Pull To Refresh')\n\n# Print summary of models in the overall UX data\nprint(\"\\nModels in normalized data:\")\nprint(model_pattern_norm[['metadata_ai_service', 'metadata_model']].drop_duplicates())\n\n# Prepare data for visualization\nnorm_comparison_data = []\n\n# Add human data\nfor idx, row in human_pattern_norm.iterrows():\n    pattern = row['metadata_pattern_type']\n    norm_comparison_data.append({\n        'pattern_type': pattern,\n        'source': 'Human',\n        'model': 'Human',\n        'overall_ux': row['human_overall_ux']\n    })\n\n# Add model data directly from model_pattern_norm\nfor idx, row in model_pattern_norm.iterrows():\n    pattern = row['metadata_pattern_type']\n    service = row['metadata_ai_service']\n    model = row['metadata_model']\n    \n    norm_comparison_data.append({\n        'pattern_type': pattern,\n        'source': service,\n        'model': model,\n        'overall_ux': row['overall_norm_ux']\n    })\n\nnorm_comparison_df = pd.DataFrame(norm_comparison_data)\n\n# Count patterns by source for verification\nprint(\"\\nPattern counts by source in normalized comparison:\")\nsource_counts = norm_comparison_df.groupby('source')['pattern_type'].count()\nprint(source_counts)\n\n# Sort by human overall UX score (worst to best)\npattern_order = human_pattern_norm.sort_values('human_overall_ux', ascending=False)['metadata_pattern_type'].tolist()\n\n# Create bar chart with specific color palette for each model\nplt.figure(figsize=(16, 10))\n\n# Set up a clear color palette\npalette = {'Human': 'royalblue', 'openai': 'darkorange', 'anthropic': 'forestgreen', 'qwen': 'darkviolet'}\n\n# Create the bar plot with ordered patterns\nax = sns.barplot(\n    x='pattern_type', \n    y='overall_ux', \n    hue='source', \n    data=norm_comparison_df, \n    palette=palette,\n    order=pattern_order\n)\n\n# Add a horizontal line at y=0 (neutral)\nplt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n\n# Customize chart\nplt.title('Normalized Overall UX Score Comparison (-3 to +3 scale)', fontsize=16)\nplt.xlabel('Pattern Type', fontsize=14)\nplt.ylabel('Overall UX Score (Higher = Worse UX, 0 = Neutral)', fontsize=14)\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Source', fontsize=12)\n\n# Set y-axis limits to show the full -3 to +3 range\nplt.ylim(-3.5, 3.5)\n\n# Add grid for better readability\nplt.grid(axis='y', linestyle='--', alpha=0.3)\n\nplt.tight_layout()\n\n# Save figure\nplt.savefig(os.path.join(output_dir, 'normalized_overall_ux_comparison.png'))\nplt.show()\n\n# Create a pivot table for heatmap\npivot_data = norm_comparison_df.pivot(index='pattern_type', columns='source', values='overall_ux')\n# Sort rows by human scores\npivot_data = pivot_data.reindex(pattern_order)\n\n# Display the data\nprint(\"\\nNormalized Overall UX Score Comparison (-3 to +3 scale):\")\nprint(pivot_data.round(2))\n\n# Create heatmap\nplt.figure(figsize=(10, 12))\n# Custom diverging colormap centered at 0\nsns.heatmap(\n    pivot_data, \n    annot=True, \n    cmap='RdBu_r',  # Red-Blue diverging map (reversed)\n    center=0,       # Center the colormap at 0\n    fmt='.2f', \n    linewidths=.5,\n    vmin=-3,        # Set min value\n    vmax=3          # Set max value\n)\nplt.title('Normalized Overall UX Score Comparison (-3 to +3 scale)', fontsize=16)\nplt.tight_layout()\n\n# Save heatmap\nplt.savefig(os.path.join(output_dir, 'normalized_ux_heatmap.png'))\nplt.show()\n\n# Save to CSV\npivot_data.round(2).reset_index().to_csv(os.path.join(output_dir, 'normalized_ux_comparison.csv'), index=False)\nprint(f'Data saved to {os.path.join(output_dir, 'normalized_ux_comparison.csv')}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}